{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e78ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "\n",
    "## @params: [JOB_NAME]\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "\n",
    "def get_glue_spark_session(catalog_nm: str, s3_bucket: str):\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(f\"spark.sql.catalog.{catalog_nm}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(f\"spark.sql.catalog.{catalog_nm}.warehouse\", s3_bucket)\n",
    "        .config(f\"spark.sql.catalog.{catalog_nm}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\")\n",
    "        .config(f\"spark.sql.catalog.{catalog_nm}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "    glue_context = GlueContext(sc)\n",
    "    return spark, glue_context\n",
    "\n",
    "def create_metadata_df(all_xlsx_files: list) -> pd.DataFrame:\n",
    "    processed = []\n",
    "\n",
    "    for item in all_xlsx_files:\n",
    "        key = item[\"Key\"]\n",
    "        last_modified = item[\"LastModified\"]\n",
    "\n",
    "        # 1. Lấy tên file (bỏ phần path, bỏ đuôi .xlsx)\n",
    "        file_name = key.split(\"/\")[-1].replace(\".xlsx\", \"\")\n",
    "\n",
    "        # 2. Trích xuất created_date (pattern YYYY-MM-DD trong path hoặc file name)\n",
    "        match = re.search(r\"\\d{4}-\\d{2}-\\d{2}\", key)\n",
    "        created_date = match.group(0) if match else None\n",
    "\n",
    "        # 3. Chuyển LastModified sang dạng date\n",
    "        last_modified_date = last_modified.date() if hasattr(last_modified, \"date\") else None\n",
    "\n",
    "        processed.append({\n",
    "            \"File_Name\": file_name,\n",
    "            \"created_date\": created_date,\n",
    "            \"LastModified\": last_modified_date\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(processed)\n",
    "\n",
    "def read_checkpoint_table(spark, catalog_nm, table_name):\n",
    "    df = spark.table(f\"{catalog_nm}.{table_name}\")\n",
    "    return df\n",
    "\n",
    "def read_all_excels_from_bucket(input_bucket: str, df_checkpoint: pd.DataFrame) -> pd.DataFrame:\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    \n",
    "    # Keep_columns\n",
    "    valid_cols = [\"today_date\", \"country\", \"platform\", \"title\", \"rank\", \"media_id\"]\n",
    "    \n",
    "    # Mapping table\n",
    "    col_mapping = {\n",
    "        \"today_date\": \"Date\",\n",
    "        \"country\": \"Market\",\n",
    "        \"platform\": \"Platform\",\n",
    "        \"title\": \"Title\",\n",
    "        \"rank\": \"Ranking\",\n",
    "        \"media_id\": \"Media ID\",\n",
    "    }\n",
    "    \n",
    "    # Scan bucket to get file .xlsx\n",
    "\n",
    "    all_xlsx_files = []\n",
    "    continuation_token = None\n",
    "\n",
    "    while True:\n",
    "        kwargs = {\"Bucket\": input_bucket}\n",
    "        if continuation_token:\n",
    "            kwargs[\"ContinuationToken\"] = continuation_token\n",
    "\n",
    "        response = s3.list_objects_v2(**kwargs)\n",
    "\n",
    "        for obj in response.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "            if key.endswith(\".xlsx\"):\n",
    "                all_xlsx_files.append({\n",
    "                    \"Key\": key,\n",
    "                    \"LastModified\": obj[\"LastModified\"]\n",
    "                })\n",
    "\n",
    "        if response.get(\"IsTruncated\"):\n",
    "            continuation_token = response[\"NextContinuationToken\"]\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    df_metadata = create_metadata_df(all_xlsx_files)\n",
    "    df_checkpoint = df_checkpoint.toPandas()\n",
    "\n",
    "    print(df_metadata.dtypes)\n",
    "    print(df_checkpoint.dtypes)\n",
    "    \"\"\"\n",
    "    df_metadata[\"created_date\"] = pd.to_datetime(df_metadata[\"created_date\"]).dt.date\n",
    "    merge = df_metadata.merge(\n",
    "        df_checkpoint,\n",
    "        left_on=[\"File_Name\", \"created_date\", \"LastModified\"],\n",
    "        right_on=[\"file_name\", \"created_date\", \"modified_date\"],\n",
    "        how=\"right\"\n",
    "    )\"\"\"\n",
    "\n",
    "\n",
    "    # Read each excel from S3\n",
    "    all_dataframes = []\n",
    "\n",
    "    for item in all_xlsx_files:\n",
    "        xlsx_key = item[\"Key\"]\n",
    "        obj = s3.get_object(Bucket=input_bucket, Key=xlsx_key)\n",
    "\n",
    "        data = obj[\"Body\"].read()\n",
    "\n",
    "        try:\n",
    "            excel_file = pd.read_excel(io.BytesIO(data), sheet_name=None, engine=\"openpyxl\")\n",
    "            print(f\"  Success: {list(excel_file.keys())}\")  # in ra tên sheet\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to read {xlsx_key}: {e}\")\n",
    "\n",
    "        df_list = []\n",
    "        for sheet_name, df in excel_file.items():\n",
    "            cols = [col for col in df.columns if col in valid_cols]\n",
    "            if cols:\n",
    "                df = df[cols]\n",
    "                df_list.append(df)\n",
    "\n",
    "        if df_list:\n",
    "            df = pd.concat(df_list, ignore_index=True)\n",
    "            df = df.rename(columns=col_mapping)\n",
    "            #df[\"source_file\"] = xlsx_key\n",
    "            all_dataframes.append(df)\n",
    "    \n",
    "    # Merge all data\n",
    "    if all_dataframes:\n",
    "        final_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "        return final_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    catalog_nm = \"glue_catalog\"\n",
    "    s3_bucket = \"s3://ampd-aldous-prod-datalake/curated\"\n",
    "\n",
    "    input_bucket = \"top-titles-raw-data\"\n",
    "    output_bucket = \"ampd-s3-auto-deletion\"\n",
    "    output_prefix = \"testing_excel_combination/\"\n",
    "\n",
    "    spark, glue_context = get_glue_spark_session(catalog_nm, s3_bucket)\n",
    "\n",
    "    job = Job(glue_context)\n",
    "    job.init(args[\"JOB_NAME\"], args)\n",
    "    \n",
    "    # Read table tmp.metadata_bucket_ttrd\n",
    "    table_name = \"tmp.metadata_bucket_ttrd\"\n",
    "    df_checkpoint = read_checkpoint_table(spark, catalog_nm, table_name)\n",
    "    \n",
    "    # Consolidate all file xlsx\n",
    "    final_df = read_all_excels_from_bucket(input_bucket, df_checkpoint)\n",
    "\n",
    "    if not final_df.empty:\n",
    "\n",
    "        # Write file CSV to S3\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_key = f\"{output_prefix.rstrip('/')}/combined_{timestamp}.csv\"\n",
    "        \n",
    "        # Convert DataFrame -> CSV (bytes format)\n",
    "        csv_buffer = io.StringIO()\n",
    "        final_df.to_csv(csv_buffer, index=False)\n",
    "        \n",
    "        # Upload to S3\n",
    "        s3 = boto3.client(\"s3\")\n",
    "        s3.put_object(\n",
    "            Bucket=output_bucket,\n",
    "            Key=output_key,\n",
    "            Body=csv_buffer.getvalue().encode(\"utf-8\"),\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    job.commit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DWH_Cole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
