{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce73a4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç ƒêang x·ª≠ l√Ω t√†i kho·∫£n C9 - T·ªïng s·ªë chi·∫øn d·ªãch: 2\n",
      "üìä L·∫•y d·ªØ li·ªáu spend: FA_Ai.Nocode_BinhND_TT_Page AI_22_04 (120223246111590236) t·ª´ 2025-04-22\n",
      "üìä L·∫•y d·ªØ li·ªáu spend: FA_AI.2025_HienPT_CVS_12_08 (120230933439750236) t·ª´ 2025-08-12\n",
      "‚úÖ ƒê√£ ghi file CSV cho C9: /home/duclu/DWH_Cole_Project/data_tmp/spend_C9_ACTIVE.csv\n",
      "\n",
      "üîç ƒêang x·ª≠ l√Ω t√†i kho·∫£n Cole8 - T·ªïng s·ªë chi·∫øn d·ªãch: 3\n",
      "üìä L·∫•y d·ªØ li·ªáu spend: FA_ML.I_BinhND_CVS_Page BOT_18_06 (120230404055340679) t·ª´ 2025-06-18\n",
      "üìä L·∫•y d·ªØ li·ªáu spend: FA_ML.I_BinhND_CVS_Page BOT_08_08 (120233571957370679) t·ª´ 2025-08-10\n",
      "üìä L·∫•y d·ªØ li·ªáu spend: FA_ML.I_BinhND_CVS_Page BOT_18_06 - B·∫£n sao (120233976431820679) t·ª´ 2025-08-19\n",
      "‚úÖ ƒê√£ ghi file CSV cho Cole8: /home/duclu/DWH_Cole_Project/data_tmp/spend_Cole8_ACTIVE.csv\n"
     ]
    }
   ],
   "source": [
    "from src.Get_data_DB import DataTransformer\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "from dotenv import load_dotenv\n",
    "import traceback\n",
    "\n",
    "# Load bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env\n",
    "load_dotenv()\n",
    "\n",
    "# Kh·ªüi t·∫°o transformer ƒë·ªÉ truy v·∫•n SQL Server\n",
    "transformer = DataTransformer()\n",
    "\n",
    "# Khai b√°o mapping gi·ªØa account v√† token ƒë√∫ng\n",
    "ACCESS_TOKENS = {\n",
    "    \"C9\": os.getenv(\"Cole_token\"),\n",
    "    \"Cole8\": os.getenv(\"BM_token\")\n",
    "}\n",
    "\n",
    "# File log l·ªói\n",
    "LOG_FILE = \"/home/duclu/DWH_Cole_Project/log_error.text\"\n",
    "\n",
    "def log_error(message: str):\n",
    "    \"\"\"Ghi l·ªói v√†o file log v·ªõi timestamp\"\"\"\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\\n\")\n",
    "\n",
    "# Truy v·∫•n danh s√°ch chi·∫øn d·ªãch ƒë√£ d·ª´ng\n",
    "paused_campaign_query = \"\"\"\n",
    "    SELECT STT AS campaign_id, \n",
    "           Chien_dich AS campaign_name,\n",
    "           Ngay_bat_dau,\n",
    "           Account\n",
    "    FROM Chien_dich_Meta\n",
    "    WHERE Account IN ('C9','Cole8')\n",
    "      AND Trang_thai = 'ACTIVE'\n",
    "\"\"\"\n",
    "try:\n",
    "    df = transformer.fetch_from_sql_server(paused_campaign_query)\n",
    "except Exception as e:\n",
    "    log_error(f\"L·ªói khi truy v·∫•n SQL: {e}\\n{traceback.format_exc()}\")\n",
    "    raise\n",
    "\n",
    "# H√†m l·∫•y chi ph√≠ theo ng√†y t·ª´ Facebook Graph API\n",
    "def fetch_campaign_spend(campaign_id, access_token, start_date, end_date):\n",
    "    url = f\"https://graph.facebook.com/v20.0/{campaign_id}/insights\"\n",
    "    params = {\n",
    "        \"access_token\": access_token,\n",
    "        \"fields\": \"spend,date_start\",\n",
    "        \"time_range\": f'{{\"since\":\"{start_date}\", \"until\":\"{end_date}\"}}',\n",
    "        \"time_increment\": 1,\n",
    "        \"limit\": 100\n",
    "    }\n",
    "    res = requests.get(url, params=params)\n",
    "    res.raise_for_status()\n",
    "    return res.json().get(\"data\", [])\n",
    "\n",
    "# L·∫•y ng√†y h√¥m nay\n",
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# X·ª≠ l√Ω cho t·ª´ng t√†i kho·∫£n: C9 v√† Cole8\n",
    "for account in [\"C9\", \"Cole8\"]:\n",
    "    access_token = ACCESS_TOKENS[account]\n",
    "    account_campaigns = df[df[\"Account\"] == account].to_dict(orient=\"records\")\n",
    "    all_rows = []\n",
    "\n",
    "    print(f\"\\nüîç ƒêang x·ª≠ l√Ω t√†i kho·∫£n {account} - T·ªïng s·ªë chi·∫øn d·ªãch: {len(account_campaigns)}\")\n",
    "\n",
    "    for campaign in account_campaigns:\n",
    "        campaign_id = campaign[\"campaign_id\"]\n",
    "        campaign_name = campaign[\"campaign_name\"]\n",
    "        start_date = campaign[\"Ngay_bat_dau\"].strftime(\"%Y-%m-%d\") if pd.notnull(campaign[\"Ngay_bat_dau\"]) else \"2024-01-01\"\n",
    "\n",
    "        try:\n",
    "            print(f\"üìä L·∫•y d·ªØ li·ªáu spend: {campaign_name} ({campaign_id}) t·ª´ {start_date}\")\n",
    "            spend_data = fetch_campaign_spend(campaign_id, access_token, start_date, today)\n",
    "            for d in spend_data:\n",
    "                all_rows.append({\n",
    "                    \"Campaign ID\": campaign_id,\n",
    "                    \"Campaign Name\": campaign_name,\n",
    "                    \"Date\": d[\"date_start\"],\n",
    "                    \"Spend\": float(d[\"spend\"])\n",
    "                })\n",
    "        except Exception as e:\n",
    "            err_msg = f\"L·ªói v·ªõi chi·∫øn d·ªãch {campaign_name} ({campaign_id}): {e}\\n{traceback.format_exc()}\"\n",
    "            print(f\"‚ö†Ô∏è {err_msg}\")\n",
    "            log_error(err_msg)\n",
    "\n",
    "    # Ghi d·ªØ li·ªáu v√†o file CSV theo t·ª´ng t√†i kho·∫£n\n",
    "    try:\n",
    "        df_spend = pd.DataFrame(all_rows)\n",
    "        output_path = os.path.expanduser(f\"~/DWH_Cole_Project/data_tmp/spend_{account}_ACTIVE.csv\")\n",
    "        df_spend.to_csv(output_path, index=False)\n",
    "        print(f\"‚úÖ ƒê√£ ghi file CSV cho {account}: {output_path}\")\n",
    "    except Exception as e:\n",
    "        log_error(f\"L·ªói khi ghi CSV cho {account}: {e}\\n{traceback.format_exc()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a17c2e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Get_data_DB import DataTransformer\n",
    "# T·∫°o instance c·ªßa class\n",
    "transformer = DataTransformer()\n",
    "\n",
    "# L·∫•y d·ªØ li·ªáu l·∫ßn ƒë·∫ßu t·ª´ nƒÉm 2024 Mysql  DATE_SUB(NOW(), INTERVAL 3 MONTH)\n",
    "mysql_query = \"\"\"SELECT DATE(DATE_ADD(l.created_at, INTERVAL 7 HOUR)) AS Thoi_gian,                                               \n",
    "                        lp.product_id AS Ma_khoa_hoc,\n",
    "                        COUNT(DISTINCT CASE WHEN s2.sale_order_level_id = 1 THEN l.id END) AS L1,\n",
    "                        COUNT(DISTINCT CASE WHEN s2.sale_order_level_id = 1 THEN l.id END) - COUNT(DISTINCT CASE WHEN s2.sale_order_level_id = 3 THEN l.id END) AS L1_L1C,\n",
    "                        COUNT(DISTINCT CASE WHEN s2.sale_order_level_id = 16 THEN l.id END) AS L7,\n",
    "                        COUNT(DISTINCT CASE WHEN s2.sale_order_level_id = 19 THEN l.id END) AS L8\n",
    "                FROM leads l \n",
    "                JOIN leads_products lp ON lp.lead_id = l.id\n",
    "                JOIN orders o ON o.lead_id = l.id\n",
    "                JOIN sale_order_histories s1 ON s1.order_id = o.id AND s1.sale_order_level_id = 1\n",
    "                JOIN sale_order_histories s2 ON s2.order_id = o.id \n",
    "                WHERE l.status = 1 \n",
    "                AND DATE(DATE_ADD(l.created_at, INTERVAL 7 HOUR)) >=  DATE_SUB(NOW(), INTERVAL 4 MONTH)\n",
    "                AND l.utm_source='FA'\n",
    "                GROUP BY Thoi_gian, Ma_khoa_hoc\n",
    "                    \"\"\"\n",
    "df = transformer.fetch_from_mysql(mysql_query)\n",
    "df.to_csv(\"~/DWH_Cole_Project/data_tmp/Count_L1_8_FA.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec9754a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25107/3008313817.py:170: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_FA = pd.concat([df_processed, df_others], ignore_index=True)\n",
      "/tmp/ipykernel_25107/3008313817.py:173: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_FA = df_FA.fillna(0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from rapidfuzz import fuzz, process\n",
    "from src.Get_data_DB import DataTransformer\n",
    "from src.Process_utm import ColumnStandardizer\n",
    "\n",
    "\n",
    "standardizer = ColumnStandardizer(\n",
    "    threshold=75,\n",
    "    preserve_if_low_similarity=[]\n",
    ")\n",
    "transformer=DataTransformer()\n",
    "\n",
    "# C√°c h√†m ƒë·ªÉ chu·∫©n ho√° t√™n kho√° h·ªçc trong t√™n chi·∫øn d·ªãch\n",
    "def build_standard_list(series):\n",
    "    \"\"\"T·∫°o danh s√°ch chu·∫©n t·ª´ Series: lo·∫°i b·ªè tr√πng v√† chu·∫©n ho√° ch·ªØ th∆∞·ªùng\"\"\"\n",
    "    clean_series = (\n",
    "        series.dropna()\n",
    "        .drop_duplicates()\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "    )\n",
    "    standard_list = clean_series.str.lower().tolist()\n",
    "    standard_map = dict(zip(clean_series.str.lower(), clean_series))\n",
    "    return standard_list, standard_map\n",
    "\n",
    "def match_course_name(value, standard_list, standard_map, threshold=60):\n",
    "    \"\"\"T√¨m match fuzzy cho 1 t√™n kho√° h·ªçc\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return \"Kh√°c\"\n",
    "\n",
    "    val = str(value).strip().lower()\n",
    "    result = process.extractOne(val, standard_list, scorer=fuzz.ratio)\n",
    "\n",
    "    if result is None:\n",
    "        return \"Kh√°c\"\n",
    "\n",
    "    match, score, _ = result\n",
    "    if score >= threshold:\n",
    "        return standard_map[match]\n",
    "    else:\n",
    "        return \"Kh√°c\"\n",
    "\n",
    "def standardize_course_column(input_series, standard_list, standard_map, threshold=60):\n",
    "    \"\"\"√Åp d·ª•ng chu·∫©n ho√° cho c·∫£ c·ªôt\"\"\"\n",
    "    return input_series.apply(lambda x: match_course_name(x, standard_list, standard_map, threshold))\n",
    "\n",
    "Query_KH=\"\"\" select Ma_khoa_hoc, Ten_khoa_hoc \n",
    "            from Dim_Khoa_hoc \"\"\"\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu\n",
    "df_KH=transformer.fetch_from_sql_server(Query_KH)\n",
    "\n",
    "df_L=pd.read_csv(\"~/DWH_Cole_Project/data_tmp/Count_L1_8_FA.csv\")\n",
    "# ƒê·ªãnh nghƒ©a c·∫•u tr√∫c DataFrame m·∫´u khi file r·ªóng\n",
    "empty_df_template = pd.DataFrame(columns=['Campaign ID', 'Campaign Name', 'Date', 'Spend'])\n",
    "\n",
    "# H√†m ki·ªÉm tra v√† ƒë·ªçc file CSV\n",
    "def read_csv_safe(file_path):\n",
    "    full_path = os.path.expanduser(file_path)\n",
    "    \n",
    "    # Ki·ªÉm tra file c√≥ t·ªìn t·∫°i v√† c√≥ k√≠ch th∆∞·ªõc > 0 byte kh√¥ng\n",
    "    if not os.path.exists(full_path) or os.path.getsize(full_path) == 0:\n",
    "        print(f\"File {file_path} r·ªóng ho·∫∑c kh√¥ng t·ªìn t·∫°i. T·∫°o DataFrame r·ªóng.\")\n",
    "        return empty_df_template.copy()\n",
    "    \n",
    "    try:\n",
    "        # Th·ª≠ ƒë·ªçc file CSV\n",
    "        df = pd.read_csv(full_path)\n",
    "        \n",
    "        # Ki·ªÉm tra n·∫øu DataFrame ƒë·ªçc ƒë∆∞·ª£c c√≥ d·ªØ li·ªáu\n",
    "        if df.empty:\n",
    "            print(f\"File {file_path} kh√¥ng c√≥ d·ªØ li·ªáu. T·∫°o DataFrame r·ªóng.\")\n",
    "            return empty_df_template.copy()\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"File {file_path} kh√¥ng c√≥ d·ªØ li·ªáu (EmptyDataError). T·∫°o DataFrame r·ªóng.\")\n",
    "        return empty_df_template.copy()\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói khi ƒë·ªçc file {file_path}: {str(e)}. T·∫°o DataFrame r·ªóng.\")\n",
    "        return empty_df_template.copy()\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n file\n",
    "\n",
    "#file_path3 = \"~/DWH_Cole_Project/data_tmp/spend_C9_PAUSED.csv\"   #L·∫•y d·ªØ li·ªáu 1 l·∫ßn duy nh·∫•t ·ªü l·∫ßn ch·∫°y ƒë·∫ßu ti√™n\n",
    "#file_path4 = \"~/DWH_Cole_Project/data_tmp/spend_Cole8_PAUSED.csv\"\n",
    "file_path1 = \"~/DWH_Cole_Project/data_tmp/spend_C9_ACTIVE.csv\"\n",
    "file_path2 = \"~/DWH_Cole_Project/data_tmp/spend_Cole8_ACTIVE.csv\"\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu\n",
    "df_cf1 = read_csv_safe(file_path1)\n",
    "df_cf2 = read_csv_safe(file_path2)\n",
    "#df_cf3=pd.read_csv(file_path3)\n",
    "#df_cf4=pd.read_csv(file_path4)\n",
    "df_cf = pd.concat([df_cf1,df_cf2], ignore_index=True)\n",
    "\n",
    "df_cf['Ten_khoa_hoc'] = df_cf['Campaign Name'].str.split('_').str[1]\n",
    "df_cf['Ma_marketer'] = df_cf['Campaign Name'].str.split('_').str[2]\n",
    "\n",
    "df_cf['Ma_marketer'] = standardizer.transform(df_cf['Ma_marketer'])\n",
    "\n",
    "\n",
    "# B∆∞·ªõc 1: T·∫°o danh s√°ch chu·∫©n\n",
    "standard_list, standard_map = build_standard_list(df_KH['Ten_khoa_hoc'].drop_duplicates())\n",
    "\n",
    "# Tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát 6 th√°ng ƒë·∫ßu nƒÉm 2025, Ai.Nocode ch·∫°y cho kho√° DTDN\n",
    "df_cf['Date'] = pd.to_datetime(df_cf['Date'])\n",
    "mask = (df_cf['Ten_khoa_hoc'] == 'Ai.Nocode') & (df_cf['Date'] >= '2025-01-01') & (df_cf['Date'] <= '2025-10-01')\n",
    "df_cf.loc[mask, 'Ten_khoa_hoc'] = 'DTDN'   \n",
    "\n",
    "df_cf['Ten_khoa_hoc'] = df_cf['Ten_khoa_hoc'].replace('BI', 'BI.01', regex=False)\n",
    "\n",
    "# B∆∞·ªõc 2: Chu·∫©n ho√° c·ªôt df_cf['Ten_khoa_hoc']\n",
    "df_cf['Ten_khoa_hoc'] = standardize_course_column(\n",
    "    df_cf['Ten_khoa_hoc'],\n",
    "    standard_list,\n",
    "    standard_map,\n",
    "    threshold=70\n",
    ")\n",
    "\n",
    "df_cf[\"Date\"] = pd.to_datetime(df_cf[\"Date\"]).dt.date\n",
    "df_L[\"Thoi_gian\"] = pd.to_datetime(df_L[\"Thoi_gian\"]).dt.date\n",
    "df_cf = df_cf.rename(columns={\n",
    "    'Spend': 'Chi_phi',\n",
    "    'Date': 'Thoi_gian'\n",
    "})\n",
    "df_cf=df_cf.merge(df_KH,on='Ten_khoa_hoc',how='inner')\n",
    "df_cf = df_cf.drop(columns=['Campaign ID', 'Campaign Name','Ten_khoa_hoc'])\n",
    "\n",
    "\n",
    "# L·∫•y nh·ªØng b·∫£n ghi tho·∫£ m√£n v√† b·∫£n ghi c√≥ chi ph√≠ nh∆∞ng kh√¥ng c√≥ s·ªë chuy·ªÉn ƒë·ªïi --> ƒê·ªï v√†o b·∫£ng Chi_phi_FA\n",
    "df_cf = df_cf.groupby(['Thoi_gian', 'Ma_khoa_hoc','Ma_marketer'])[['Chi_phi']].sum().reset_index()\n",
    "\n",
    "df_FA=df_cf.merge(df_L,on=['Thoi_gian','Ma_khoa_hoc'], how='left')\n",
    "\n",
    "# V√¨ s·∫£n ph·∫©m DE.COMBO01 c√≥ 2 m√£ kho√° n√™n ·ªü b∆∞·ªõc √°nh x·∫° t·ª´ Ten_khoa_hoc th√†nh Ma_khoa_hoc ph√°t sinh th√†nh 2 b·∫£n ghi Chi ph√≠ =\n",
    "# 1. T√°ch d·ªØ li·ªáu c·∫ßn x·ª≠ l√Ω v√† ph·∫ßn c√≤n l·∫°i\n",
    "df_to_process = df_FA[df_FA['Ma_khoa_hoc'].isin([515, 550])]\n",
    "df_others = df_FA[~df_FA['Ma_khoa_hoc'].isin([515, 550])]\n",
    "\n",
    "# 2. N·∫øu c√≥ b·∫£n ghi c·∫ßn x·ª≠ l√Ω\n",
    "if not df_to_process.empty:\n",
    "    def xu_ly_nhom(gr):\n",
    "        if len(gr) == 2:\n",
    "            na_mask = gr['L1'].isna()\n",
    "            if na_mask.sum() == 1:\n",
    "                return gr[~na_mask]\n",
    "            elif na_mask.sum() == 2:\n",
    "                if (gr['Ma_khoa_hoc'] == 515).any():\n",
    "                    return gr[gr['Ma_khoa_hoc'] == 515]\n",
    "                else:\n",
    "                    return pd.DataFrame()\n",
    "            else:\n",
    "                return gr\n",
    "        else:\n",
    "            return gr[~gr['L1'].isna()] if gr['L1'].isna().any() else gr\n",
    "\n",
    "    df_processed = (\n",
    "        df_to_process\n",
    "        .groupby(['Chi_phi', 'Thoi_gian'], group_keys=False)\n",
    "        .apply(xu_ly_nhom)\n",
    "    )\n",
    "else:\n",
    "    # Kh√¥ng c√≥ g√¨ ƒë·ªÉ x·ª≠ l√Ω\n",
    "    df_processed = pd.DataFrame(columns=df_FA.columns)\n",
    "\n",
    "# 3. Gh√©p l·∫°i d·ªØ li·ªáu\n",
    "df_FA = pd.concat([df_processed, df_others], ignore_index=True)\n",
    "\n",
    "\n",
    "df_FA = df_FA.fillna(0)\n",
    "df_L78=df_FA[['Thoi_gian','Ma_khoa_hoc','Ma_marketer','L7','L8']]\n",
    "df_FA=df_FA[['Thoi_gian','Ma_khoa_hoc','Ma_marketer','Chi_phi','L1','L1_L1C']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9ee4407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tr∆∞·ªùng h·ª£p c√≥ s·ªë L1 chuy·ªÉn ƒë·ªïi  nh∆∞ng kh√¥ng c√≥ chi ph√≠ th√¨ chuy·ªÉn th√†nh s·ªë L1_mess ·ªü b·∫£ng Chi_phi_mess \n",
    "df_mess_bonus=df_cf.merge(df_L, on=['Ma_khoa_hoc', 'Thoi_gian'], how='right')\n",
    "df_mess_bonus=df_mess_bonus[['Thoi_gian','Chi_phi','Ma_khoa_hoc','Ma_marketer','L1','L1_L1C','L7','L8']]\n",
    "df_mess_bonus = df_mess_bonus[df_mess_bonus['Chi_phi'].isna()]\n",
    "df_mess_bonus['Chi_phi'] = df_mess_bonus['Chi_phi'].fillna(0)\n",
    "\n",
    "\n",
    "# Ghi d·ªØ li·ªáu v√†o folder data_result\n",
    "df_FA.to_csv(\"~/DWH_Cole_Project/data_result/Chi_phi_FA_transformed.csv\", index=False)\n",
    "df_L78.to_csv(\"~/DWH_Cole_Project/data_result/L78_FA_transformed.csv\", index=False)\n",
    "df_mess_bonus.to_csv(\"~/DWH_Cole_Project/data_result/Chi_phi_mess_bonus_transformed.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DWH_Cole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
