{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178af890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.Get_data_DB import DataTransformer\n",
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "transform=DataTransformer()\n",
    "\n",
    "Product_query='select Ma_khoa_hoc, Ten_khoa_hoc from Dim_Khoa_hoc where Ma_khoa_hoc != 550'\n",
    "Class_query='select Ma_lop_hoc, Ten_lop_hoc from Dim_Lop_hoc'\n",
    "\n",
    "df_product=transform.fetch_from_sql_server(Product_query)\n",
    "df_class=transform.fetch_from_sql_server(Class_query)\n",
    "\n",
    "\n",
    "\n",
    "df1=pd.read_csv(\"~/DWH_Cole_Project/data_tmp/Ke_hoach_Sale_TOA.csv\")\n",
    "df2=pd.read_csv(\"~/DWH_Cole_Project/data_tmp/Ke_hoach_Sale_TOT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Các hàm để đảm bảo tên khoá học chuẩn trong file kế hoạch\n",
    "def build_standard_list(series):\n",
    "    \"\"\"Tạo danh sách chuẩn từ Series: loại bỏ trùng và chuẩn hoá chữ thường\"\"\"\n",
    "    clean_series = (\n",
    "        series.dropna()\n",
    "        .drop_duplicates()\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "    )\n",
    "    standard_list = clean_series.str.lower().tolist()\n",
    "    standard_map = dict(zip(clean_series.str.lower(), clean_series))\n",
    "    return standard_list, standard_map\n",
    "\n",
    "def match_course_name(value, standard_list, standard_map, threshold=60):\n",
    "    \"\"\"Tìm match fuzzy cho 1 tên khoá học\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return \"Khác\"\n",
    "\n",
    "    val = str(value).strip().lower()\n",
    "    result = process.extractOne(val, standard_list, scorer=fuzz.ratio)\n",
    "\n",
    "    if result is None:\n",
    "        return \"Khác\"\n",
    "\n",
    "    match, score, _ = result\n",
    "    if score >= threshold:\n",
    "        return standard_map[match]\n",
    "    else:\n",
    "        return \"Khác\"\n",
    "\n",
    "def standardize_course_column(input_series, standard_list, standard_map, threshold=60):\n",
    "    \"\"\"Áp dụng chuẩn hoá cho cả cột\"\"\"\n",
    "    return input_series.apply(lambda x: match_course_name(x, standard_list, standard_map, threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.rename(columns={\n",
    "    \"Dữ liệu kế hoạch theo tháng\":\"Nam\",\n",
    "    \"Unnamed: 1\":\"Thang\",\n",
    "    \"Unnamed: 2\": \"Ten_khoa_hoc\",\n",
    "    \"Unnamed: 3\": \"Thu_tu_lop\",\n",
    "    \"Unnamed: 4\":\"L1\",\n",
    "    \"Unnamed: 5\": \"L1_L1/Sale\",\n",
    "    \"Unnamed: 6\": \"L1_L1C\",\n",
    "    \"Unnamed: 7\": \"L2\",\n",
    "    \"Unnamed: 8\":\"Khong xac dinh\",\n",
    "    \"Unnamed: 9\": \"L7\",\n",
    "    \"Unnamed: 10\":\"L8\",\n",
    "    \"Unnamed: 11\":\"L7+L8\",\n",
    "    \"Unnamed: 12\":\"Tuan 1\",\n",
    "    \"Unnamed: 13\":\"Tuan 2\",\n",
    "    \"Unnamed: 14\":\"Tuan 3\",\n",
    "    \"Unnamed: 15\":\"Tuan 4\",\n",
    "    \"Unnamed: 16\":\"Tuan 5\",\n",
    "    \"Unnamed: 17\":\"Doanh_so\",\n",
    "    \"Unnamed: 18\":\"Doanh_so/Sale\",\n",
    "})\n",
    "df1=df1.drop(columns=[\"L7+L8\",\"Khong xac dinh\",\"L1_L1C\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e3b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Nam']=df1['Nam'].ffill()\n",
    "df1['Thang']=df1['Thang'].ffill()\n",
    "df1['Ten_khoa_hoc']=df1['Ten_khoa_hoc'].ffill()\n",
    "df1=df1.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc1d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Thang'] = df1['Thang'].str.extract(r'(\\d+)')[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Ten_khoa_hoc'] = df1['Ten_khoa_hoc'].str.replace('\\n', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646631fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Ten_khoa_hoc'] = df1['Ten_khoa_hoc'].replace(['Khóa DS, DA ','Khóa DS, DA'], 'DS.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46500d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In hoa cột\n",
    "df1['Thu_tu_lop'] = df1['Thu_tu_lop'].str.upper()\n",
    "\n",
    "# Trích xuất mã K\n",
    "df1['Thu_tu_lop'] = df1['Thu_tu_lop'].str.extract(r'(K\\d+)')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dc7e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.dropna(subset=['Thu_tu_lop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1cc369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tìm mã K có độ dài = 2 và thêm số 0\n",
    "mask = (df1['Thu_tu_lop'].str.len() == 2) & df1['Thu_tu_lop'].notna()\n",
    "df1.loc[mask, 'Thu_tu_lop'] = df1.loc[mask, 'Thu_tu_lop'].str.replace(r'K(\\d)', r'K0\\1', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c88bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bước 1: Tạo danh sách chuẩn\n",
    "standard_list, standard_map = build_standard_list(df_product['Ten_khoa_hoc'].drop_duplicates())\n",
    "\n",
    "# Bước 2: Chuẩn hoá cột df_cf['Ten_khoa_hoc']\n",
    "df1['Ten_khoa_hoc'] = standardize_course_column(\n",
    "    df1['Ten_khoa_hoc'],\n",
    "    standard_list,\n",
    "    standard_map,\n",
    "    threshold=75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Ten_lop_hoc'] = df1['Ten_khoa_hoc'].astype(str) + '-' + df1['Thu_tu_lop'].astype(str)\n",
    "df1=df1.drop(columns=['Ten_khoa_hoc','Thu_tu_lop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a393cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.merge(df_class,on='Ten_lop_hoc', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e9473",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'[.đ\\s]'  # Loại bỏ dấu chấm, chữ đ và khoảng trắng\n",
    "df1['Doanh_so'] = df1['Doanh_so'].str.replace(pattern, '', regex=True)\n",
    "df1['Doanh_so/Sale'] = df1['Doanh_so/Sale'].str.replace(pattern, '', regex=True)\n",
    "df1=df1.drop(columns='Ten_lop_hoc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b15dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bdd22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dữ liệu TOA theo Tháng\n",
    "cols_thang = ['Nam', 'Thang', 'L1', 'L1_L1/Sale', 'L2','L7', 'L8', 'Doanh_so', 'Doanh_so/Sale', 'Ma_lop_hoc']\n",
    "df_thang = df1[cols_thang].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee98fa93",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13177779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15750/3116744903.py:163: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(xu_ly_nhom)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from rapidfuzz import fuzz, process\n",
    "from src.Get_data_DB import DataTransformer\n",
    "from src.Process_utm import ColumnStandardizer\n",
    "\n",
    "\n",
    "standardizer = ColumnStandardizer(\n",
    "    threshold=75,\n",
    "    preserve_if_low_similarity=[]\n",
    ")\n",
    "transformer=DataTransformer()\n",
    "\n",
    "# Các hàm để chuẩn hoá tên khoá học trong tên chiến dịch\n",
    "def build_standard_list(series):\n",
    "    \"\"\"Tạo danh sách chuẩn từ Series: loại bỏ trùng và chuẩn hoá chữ thường\"\"\"\n",
    "    clean_series = (\n",
    "        series.dropna()\n",
    "        .drop_duplicates()\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "    )\n",
    "    standard_list = clean_series.str.lower().tolist()\n",
    "    standard_map = dict(zip(clean_series.str.lower(), clean_series))\n",
    "    return standard_list, standard_map\n",
    "\n",
    "def match_course_name(value, standard_list, standard_map, threshold=60):\n",
    "    \"\"\"Tìm match fuzzy cho 1 tên khoá học\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return \"Khác\"\n",
    "\n",
    "    val = str(value).strip().lower()\n",
    "    result = process.extractOne(val, standard_list, scorer=fuzz.ratio)\n",
    "\n",
    "    if result is None:\n",
    "        return \"Khác\"\n",
    "\n",
    "    match, score, _ = result\n",
    "    if score >= threshold:\n",
    "        return standard_map[match]\n",
    "    else:\n",
    "        return \"Khác\"\n",
    "\n",
    "def standardize_course_column(input_series, standard_list, standard_map, threshold=60):\n",
    "    \"\"\"Áp dụng chuẩn hoá cho cả cột\"\"\"\n",
    "    return input_series.apply(lambda x: match_course_name(x, standard_list, standard_map, threshold))\n",
    "\n",
    "Query_KH=\"\"\" select Ma_khoa_hoc, Ten_khoa_hoc \n",
    "            from Dim_Khoa_hoc \"\"\"\n",
    "\n",
    "# Đọc dữ liệu\n",
    "df_KH=transformer.fetch_from_sql_server(Query_KH)\n",
    "\n",
    "df_L78=pd.read_csv(\"~/DWH_Cole_Project/data_tmp/L78_FA_Marketing.csv\")\n",
    "# Định nghĩa cấu trúc DataFrame mẫu khi file rỗng\n",
    "empty_df_template = pd.DataFrame(columns=['Campaign ID', 'Campaign Name', 'Date', 'Spend'])\n",
    "\n",
    "# Hàm kiểm tra và đọc file CSV\n",
    "def read_csv_safe(file_path):\n",
    "    full_path = os.path.expanduser(file_path)\n",
    "    \n",
    "    # Kiểm tra file có tồn tại và có kích thước > 0 byte không\n",
    "    if not os.path.exists(full_path) or os.path.getsize(full_path) == 0:\n",
    "        print(f\"File {file_path} rỗng hoặc không tồn tại. Tạo DataFrame rỗng.\")\n",
    "        return empty_df_template.copy()\n",
    "    \n",
    "    try:\n",
    "        # Thử đọc file CSV\n",
    "        df = pd.read_csv(full_path)\n",
    "        \n",
    "        # Kiểm tra nếu DataFrame đọc được có dữ liệu\n",
    "        if df.empty:\n",
    "            print(f\"File {file_path} không có dữ liệu. Tạo DataFrame rỗng.\")\n",
    "            return empty_df_template.copy()\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"File {file_path} không có dữ liệu (EmptyDataError). Tạo DataFrame rỗng.\")\n",
    "        return empty_df_template.copy()\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file {file_path}: {str(e)}. Tạo DataFrame rỗng.\")\n",
    "        return empty_df_template.copy()\n",
    "\n",
    "# Đường dẫn file\n",
    "\n",
    "file_path3 = \"~/DWH_Cole_Project/data_tmp/spend_C9_PAUSED.csv\"   #Lấy dữ liệu 1 lần duy nhất ở lần chạy đầu tiên\n",
    "file_path4 = \"~/DWH_Cole_Project/data_tmp/spend_Cole8_PAUSED.csv\"\n",
    "file_path1 = \"~/DWH_Cole_Project/data_tmp/spend_C9_ACTIVE.csv\"\n",
    "file_path2 = \"~/DWH_Cole_Project/data_tmp/spend_Cole8_ACTIVE.csv\"\n",
    "\n",
    "# Đọc dữ liệu\n",
    "df_cf1 = read_csv_safe(file_path1)\n",
    "df_cf2 = read_csv_safe(file_path2)\n",
    "df_cf3=pd.read_csv(file_path3)\n",
    "df_cf4=pd.read_csv(file_path4)\n",
    "df_cf = pd.concat([df_cf1,df_cf2,df_cf3,df_cf4], ignore_index=True)\n",
    "\n",
    "df_cf['Ten_khoa_hoc'] = df_cf['Campaign Name'].str.split('_').str[1]\n",
    "df_cf['Ma_marketer'] = df_cf['Campaign Name'].str.split('_').str[2]\n",
    "\n",
    "df_cf['Ma_marketer'] = standardizer.transform(df_cf['Ma_marketer'])\n",
    "\n",
    "\n",
    "# Bước 1: Tạo danh sách chuẩn\n",
    "standard_list, standard_map = build_standard_list(df_KH['Ten_khoa_hoc'].drop_duplicates())\n",
    "\n",
    "# Trường hợp đặc biệt 6 tháng đầu năm 2025, Ai.Nocode chạy cho khoá DTDN\n",
    "df_cf['Date'] = pd.to_datetime(df_cf['Date'])\n",
    "mask = (df_cf['Ten_khoa_hoc'] == 'Ai.Nocode') & (df_cf['Date'] >= '2025-01-01') & (df_cf['Date'] <= '2025-10-01')\n",
    "df_cf.loc[mask, 'Ten_khoa_hoc'] = 'DTDN'   \n",
    "\n",
    "df_cf['Ten_khoa_hoc'] = df_cf['Ten_khoa_hoc'].replace('BI', 'BI.01', regex=False)\n",
    "\n",
    "# Bước 2: Chuẩn hoá cột df_cf['Ten_khoa_hoc']\n",
    "df_cf['Ten_khoa_hoc'] = standardize_course_column(\n",
    "    df_cf['Ten_khoa_hoc'],\n",
    "    standard_list,\n",
    "    standard_map,\n",
    "    threshold=70\n",
    ")\n",
    "\n",
    "df_cf[\"Date\"] = pd.to_datetime(df_cf[\"Date\"]).dt.date\n",
    "df_L78[\"Thoi_gian\"] = pd.to_datetime(df_L78[\"Thoi_gian\"]).dt.date\n",
    "df_cf = df_cf.rename(columns={\n",
    "    'Spend': 'Chi_phi',\n",
    "    'Date': 'Thoi_gian'\n",
    "})\n",
    "df_cf=df_cf.merge(df_KH,on='Ten_khoa_hoc',how='inner')\n",
    "df_cf = df_cf.drop(columns=['Campaign ID', 'Campaign Name','Ten_khoa_hoc'])\n",
    "\n",
    "\n",
    "# Lấy những bản ghi thoả mãn và bản ghi có chi phí nhưng không có số chuyển đổi --> Đổ vào bảng Chi_phi_FA\n",
    "df_cf = df_cf.groupby(['Thoi_gian', 'Ma_khoa_hoc','Ma_marketer'])[['Chi_phi']].sum().reset_index()\n",
    "\n",
    "df_FA=df_cf.merge(df_L78,on=['Thoi_gian','Ma_khoa_hoc'], how='left')\n",
    "\n",
    "# Vì sản phẩm DE.COMBO01 có 2 mã khoá nên ở bước ánh xạ từ Ten_khoa_hoc thành Ma_khoa_hoc phát sinh thành 2 bản ghi Chi phí =\n",
    "# 1. Tách dữ liệu cần xử lý và phần còn lại\n",
    "df_to_process = df_FA[df_FA['Ma_khoa_hoc'].isin([515, 550])]\n",
    "df_others = df_FA[~df_FA['Ma_khoa_hoc'].isin([515, 550])]\n",
    "\n",
    "# 2. Nếu có bản ghi cần xử lý\n",
    "if not df_to_process.empty:\n",
    "    def xu_ly_nhom(gr):\n",
    "        if len(gr) == 2:\n",
    "            na_mask = gr['L7'].isna()\n",
    "            if na_mask.sum() == 1:\n",
    "                return gr[~na_mask]\n",
    "            elif na_mask.sum() == 2:\n",
    "                if (gr['Ma_khoa_hoc'] == 515).any():\n",
    "                    return gr[gr['Ma_khoa_hoc'] == 515]\n",
    "                else:\n",
    "                    return pd.DataFrame()\n",
    "            else:\n",
    "                return gr\n",
    "        else:\n",
    "            return gr[~gr['L7'].isna()] if gr['L7'].isna().any() else gr\n",
    "\n",
    "    df_processed = (\n",
    "        df_to_process\n",
    "        .groupby(['Chi_phi', 'Thoi_gian'], group_keys=False)\n",
    "        .apply(xu_ly_nhom)\n",
    "    )\n",
    "else:\n",
    "    # Không có gì để xử lý\n",
    "    df_processed = pd.DataFrame(columns=df_FA.columns)\n",
    "\n",
    "# 3. Ghép lại dữ liệu\n",
    "df_FA = pd.concat([df_processed, df_others], ignore_index=True)\n",
    "df_FA = df_FA.fillna(0)\n",
    "df_FA=df_FA.drop(columns='Chi_phi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d770ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trường hợp có số L7 chuyển đổi  nhưng không có chi phí thì chuyển thành số L7_mess ở bảng Chi_phi_mess \n",
    "df_mess_bonus=df_cf.merge(df_L78, on=['Ma_khoa_hoc', 'Thoi_gian'], how='right')\n",
    "df_mess_bonus=df_mess_bonus[['Thoi_gian','Chi_phi','Ma_khoa_hoc','Ma_marketer','L7','L8']]\n",
    "df_mess_bonus = df_mess_bonus[df_mess_bonus['Chi_phi'].isna()]\n",
    "df_mess_bonus.drop(columns='Chi_phi')\n",
    "\n",
    "\n",
    "# Ghi dữ liệu vào folder data_result\n",
    "df_FA.to_csv(\"~/DWH_Cole_Project/data_result/L78_FA_transformed.csv\", index=False)\n",
    "df_mess_bonus.to_csv(\"~/DWH_Cole_Project/data_result/L78_mess_bonus_transformed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd7e19",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e570f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1=pd.read_csv(\"~/DWH_Cole_Project/data_tmp/So_L1C_Mess_TOT.csv\")\n",
    "df2=pd.read_csv(\"~/DWH_Cole_Project/data_tmp/So_L1_Mess_TOT.csv\")\n",
    "df3=pd.read_csv(\"~/DWH_Cole_Project/data_tmp/So_L7_Mess_TOT.csv\")\n",
    "df4=pd.read_csv(\"~/DWH_Cole_Project/data_tmp/So_L8_Mess_TOT.csv\")\n",
    "\n",
    "df_bonus=pd.read_csv(\"~/DWH_Cole_Project/data_result/Chi_phi_mess_bonus_transformed.csv\")\n",
    "\n",
    "\n",
    "# Chuẩn hoá dữ liệu \n",
    "merged=df1.merge(df2,on=['Thoi_gian','Ma_khoa_hoc'], how=\"outer\")\n",
    "merged=merged.merge(df3,on=['Thoi_gian','Ma_khoa_hoc'], how=\"outer\")\n",
    "merged=merged.merge(df4,on=['Thoi_gian','Ma_khoa_hoc'], how=\"outer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5d8fd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thoi_gian</th>\n",
       "      <th>Ma_khoa_hoc</th>\n",
       "      <th>L1C</th>\n",
       "      <th>L1</th>\n",
       "      <th>L7</th>\n",
       "      <th>L8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>515</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>517</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>529</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>548</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-02</td>\n",
       "      <td>515</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>554</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>2025-07-31</td>\n",
       "      <td>517</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>2025-07-31</td>\n",
       "      <td>548</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>2025-07-31</td>\n",
       "      <td>553</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1064 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Thoi_gian  Ma_khoa_hoc  L1C   L1   L7   L8\n",
       "0     2025-01-01          515  1.0  2.0  NaN  NaN\n",
       "1     2025-01-01          517  1.0  4.0  NaN  NaN\n",
       "2     2025-01-01          529  1.0  1.0  NaN  NaN\n",
       "3     2025-01-01          548  3.0  3.0  NaN  NaN\n",
       "4     2025-01-02          515  NaN  3.0  NaN  NaN\n",
       "...          ...          ...  ...  ...  ...  ...\n",
       "1059  2025-07-30          548  NaN  5.0  1.0  1.0\n",
       "1060  2025-07-30          554  1.0  1.0  NaN  NaN\n",
       "1061  2025-07-31          517  1.0  1.0  NaN  NaN\n",
       "1062  2025-07-31          548  2.0  4.0  NaN  NaN\n",
       "1063  2025-07-31          553  3.0  6.0  NaN  NaN\n",
       "\n",
       "[1064 rows x 6 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6099ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Thay thế giá trị trống (NaN) trong cột 'L1C' bằng 0\n",
    "# inplace=True sẽ thay đổi trực tiếp trên DataFrame 'merged' mà không cần gán lại\n",
    "merged['L1C'] = merged['L1C'].fillna(0)\n",
    "\n",
    "# 2. Tạo cột mới 'L1_L1C' bằng L1 - L1C\n",
    "# Đảm bảo rằng cả L1 và L1C đều là kiểu số trước khi thực hiện phép trừ\n",
    "# Nếu chúng có thể là kiểu object (chuỗi), bạn cần chuyển đổi chúng sang số trước.\n",
    "merged['L1'] = pd.to_numeric(merged['L1'], errors='coerce')\n",
    "merged['L1C'] = pd.to_numeric(merged['L1C'], errors='coerce')\n",
    "\n",
    "merged['L1_L1C'] = merged['L1'] - merged['L1C']\n",
    "\n",
    "merged=merged.drop(columns=['L1C'])\n",
    "\n",
    "#----Cộng thêm dữ liệu L1 bonus \n",
    "df_bonus=df_bonus[['Thoi_gian','Ma_khoa_hoc','L1','L1_L1C','L7','L8']]\n",
    "df_combined = pd.concat([df_bonus, merged], ignore_index=True)\n",
    "df_result = df_combined.groupby(['Thoi_gian', 'Ma_khoa_hoc'])[['L1', 'L1_L1C','L7','L8']].sum().reset_index()\n",
    "df_result = df_result[df_result['Thoi_gian'] >= '2025-01-01']\n",
    "df_result.to_csv(\"~/DWH_Cole_Project/data_result/Count_L_of_Mess.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "42c8b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_excel(\"/tmp/Checks.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a8a24",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9741fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.Get_data_DB import DataTransformer\n",
    "transformer = DataTransformer()\n",
    "\n",
    "Offline_class_query = \"\"\" select Ma_lop_hoc,\n",
    "                                 Ngay_khai_giang,\n",
    "                                 So_buoi_tuyen_sinh\n",
    "                          from Dim_Lop_hoc\n",
    "\"\"\"\n",
    "# Có 2 cách tiếp cận phân nhánh lớp thuộc khoá học. Cách 1: Lớp thuộc nhiều sản phẩm ( sản phẩm lẻ và sản phẩm combo). \n",
    "# Cách 2: Lớp thuộc về 1 sản phẩm suy nhất . Trong Dim đang tổ chức theo cách hai nhưng từ Tên khoá học của Chiến dịch Meta muốn \n",
    "# về lớp thì phải dùng Cách 1\n",
    "Mapping_class_query=\"\"\"select oc.id Ma_lop_hoc,\n",
    "                               p.id Ma_khoa_hoc\n",
    "                        from offline_classes oc\n",
    "                        join product_items pi on pi.item_id =oc.item_id\n",
    "                        join products p on pi.product_id =p.id\"\"\"\n",
    "df_LH = transformer.fetch_from_sql_server(Offline_class_query)\n",
    "df_mapping=transformer.fetch_from_mysql(Mapping_class_query)\n",
    "df= pd.read_csv(\"~/DWH_Cole_Project/data_result/Chi_phi_FA_transformed.csv\")\n",
    "df_78=pd.read_csv(\"~/DWH_Cole_Project/data_result/L78_FA_transformed.csv\")\n",
    "\n",
    "# Gộp lại dữ liệu L7, L8 vào dữ liệu gốc\n",
    "df=df.merge(df_78, on=['Thoi_gian','Ma_khoa_hoc','Ma_marketer'],how='outer')\n",
    "df=df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ec52a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('/tmp/Check1.xlsx',"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f3261",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34afd209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.Get_data_DB import DataTransformer\n",
    "transformer = DataTransformer()\n",
    "\n",
    "Offline_class_query = \"\"\" select Ma_lop_hoc,\n",
    "                                 Ngay_khai_giang,\n",
    "                                 So_buoi_tuyen_sinh\n",
    "                          from Dim_Lop_hoc\n",
    "\"\"\"\n",
    "# Có 2 cách tiếp cận phân nhánh lớp thuộc khoá học. Cách 1: Lớp thuộc nhiều sản phẩm ( sản phẩm lẻ và sản phẩm combo). \n",
    "# Cách 2: Lớp thuộc về 1 sản phẩm suy nhất . Trong Dim đang tổ chức theo cách hai nhưng từ Tên khoá học của Chiến dịch Meta muốn \n",
    "# về lớp thì phải dùng Cách 1\n",
    "Mapping_class_query=\"\"\"select oc.id Ma_lop_hoc,\n",
    "                               p.id Ma_khoa_hoc\n",
    "                        from offline_classes oc\n",
    "                        join product_items pi on pi.item_id =oc.item_id\n",
    "                        join products p on pi.product_id =p.id\"\"\"\n",
    "df_LH = transformer.fetch_from_sql_server(Offline_class_query)\n",
    "df_mapping=transformer.fetch_from_mysql(Mapping_class_query)\n",
    "df= pd.read_csv(\"~/DWH_Cole_Project/data_result/Chi_phi_FA_transformed.csv\")\n",
    "df_78=pd.read_csv(\"~/DWH_Cole_Project/data_result/L78_FA_transformed.csv\")\n",
    "\n",
    "# Gộp lại dữ liệu L7, L8 vào dữ liệu gốc\n",
    "df=df.merge(df_78, on=['Thoi_gian','Ma_khoa_hoc','Ma_marketer'],how='outer')\n",
    "\n",
    "# Join để bảng ánh xa Lớp thuộc những sản phẩm nào ( 1 Lớp sẽ thuộc nhiều sản phẩm, đặc biệt sản phẩm bán có COMBO)\n",
    "df_LH=df_LH.merge(df_mapping, on='Ma_lop_hoc', how='inner')\n",
    "df['Thoi_gian'] = pd.to_datetime(df['Thoi_gian'])\n",
    "df_LH['Ngay_khai_giang'] = pd.to_datetime(df_LH['Ngay_khai_giang'])\n",
    "\n",
    "# Join dữ liệu để Chi phí về lớp\n",
    "merged = df.merge(df_LH,on='Ma_khoa_hoc', how='inner')\n",
    "\n",
    "# Điều kiện 1: So_buoi_tuyen_sinh là 0 hoặc 1 và Thoi_gian <= Ngay_khai_giang\n",
    "# Đây là trường hợp sản phẩm đó mới chỉ mở 1 lớp hoặc nó thuộc lớp đầu tiên của sản phẩm đó\n",
    "cond1 = (merged[\"So_buoi_tuyen_sinh\"].isin([0, 1])) & (merged[\"Thoi_gian\"] <= merged[\"Ngay_khai_giang\"])\n",
    "\n",
    "# Điều kiện 2: So_buoi_tuyen_sinh khác -1 và Thoi_gian nằm trong khoảng (Ngay_khai_giang - So_buoi_tuyen_sinh, Ngay_khai_giang)\n",
    "# Dữ liệu CHi phí được lấy từ 2014, từ lúc này Ngày khai giảng của các lớp đều được điền đầy đủ\n",
    "cond2 = (~merged[\"So_buoi_tuyen_sinh\"].isin([-1])) & (\n",
    "    merged[\"Thoi_gian\"] > (merged[\"Ngay_khai_giang\"] - pd.to_timedelta(merged[\"So_buoi_tuyen_sinh\"], unit=\"D\"))\n",
    ") & (merged[\"Thoi_gian\"] <= merged[\"Ngay_khai_giang\"])\n",
    "\n",
    "# Lọc các bản ghi thỏa mãn điều kiện\n",
    "valid_records = merged[cond1 | cond2]\n",
    "\n",
    "# Xử lí riêng cho trường hợp Chi phí chạy cho sản phẩm COMBO, khi này 1 chi phí bị lặp lại cho nhiều lớp thuộc sản phẩm đó\n",
    "# Bước 1: Nhóm dữ liệu theo Thoi_gian và Ma_khoa_hoc\n",
    "grouped = valid_records.groupby(['Thoi_gian', 'Ma_khoa_hoc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "125d3ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20364/1565446434.py:33: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  valid_records_processed = grouped.apply(adjust_group_values)\n"
     ]
    }
   ],
   "source": [
    "# Bước 2: Tạo hàm xử lý cho mỗi nhóm\n",
    "def adjust_group_values(group):\n",
    "    count = len(group)\n",
    "    if count > 1:\n",
    "        # Lấy giá trị đầu tiên của mỗi cột\n",
    "        first_values = {\n",
    "            'Chi_phi': group['Chi_phi'].iloc[0],\n",
    "            'L1': group['L1'].iloc[0],\n",
    "            'L1_L1C': group['L1_L1C'].iloc[0],\n",
    "            'L7': group['L7'].iloc[0],\n",
    "            'L8': group['L8'].iloc[0]\n",
    "        }\n",
    "        \n",
    "        # Tính giá trị mới cho mỗi cột và làm tròn\n",
    "        adjusted_values = {\n",
    "            'Chi_phi': round(first_values['Chi_phi'] / count),\n",
    "            'L1': round(first_values['L1'] / count),\n",
    "            'L1_L1C': round(first_values['L1_L1C'] / count),\n",
    "            'L7': round(first_values['L7'] / count),\n",
    "            'L8': round(first_values['L8'] / count)\n",
    "        }\n",
    "        \n",
    "        # Gán giá trị mới cho tất cả các bản ghi trong nhóm\n",
    "        group['Chi_phi'] = adjusted_values['Chi_phi']\n",
    "        group['L1'] = adjusted_values['L1']\n",
    "        group['L1_L1C'] = adjusted_values['L1_L1C']\n",
    "        group['L7'] = adjusted_values['L7']\n",
    "        group['L8'] = adjusted_values['L8']\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Bước 3: Áp dụng hàm xử lý cho từng nhóm\n",
    "valid_records_processed = grouped.apply(adjust_group_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41326b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bước 4: Reset index nếu cần (do groupby tạo multi-index)\n",
    "valid_records_processed = valid_records_processed.reset_index(drop=True)\n",
    "\n",
    "# Lấy các cột cần thiết\n",
    "final_L78=valid_records_processed[['Thoi_gian','Ma_lop_hoc','Ma_marketer','L7','L8']]\n",
    "final=valid_records_processed[['Chi_phi','Thoi_gian','Ma_lop_hoc','Ma_marketer','L1','L1_L1C']]\n",
    "\n",
    "final.to_csv(\"~/DWH_Cole_Project/data_result/Chi_phi_FA_TOA_transformed.csv\",index=False)\n",
    "final_L78.to_csv(\"~/DWH_Cole_Project/data_result/L78_FA_TOA_transformed.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DWH_Cole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
